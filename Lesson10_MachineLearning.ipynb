{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lesson10_MachineLearning.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andreluisvsa/mlbsb/blob/master/Lesson10_MachineLearning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "rEj3xt7RYMN_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Caderno de apoio para o vídeo 10 do curso de Machine Learning da Fast.ai - NLP"
      ]
    },
    {
      "metadata": {
        "id": "YAIvUurP1Qg5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "[Vídeo completo da lição 10 ](http://course.fast.ai/lessonsml1/lesson10.html)"
      ]
    },
    {
      "metadata": {
        "id": "1ki-r3G11nOb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Link para Datasets utilizados, caderno do Jeremy e para exemplos do vídeo: "
      ]
    },
    {
      "metadata": {
        "id": "t1TMTCtn1yBa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "[Dataset for English Words](https://www.kaggle.com/c/text-normalization-challenge-english-language)"
      ]
    },
    {
      "metadata": {
        "id": "ab5YZdDd2L_Q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "[Exemplo do Kaggle](https://www.kaggle.com/alvira12/class-wise-processing-lb-0-992-new-dataset)"
      ]
    },
    {
      "metadata": {
        "id": "dRpQAP9y2W0T",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "[Exemplo 2 do Kaggle](https://www.kaggle.com/neerjad/class-wise-regex-functions-l-b-0-995)"
      ]
    },
    {
      "metadata": {
        "id": "QKV6SOjk3EXk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "[Exemplo 3 do Kaggle ](https://www.kaggle.com/c/porto-seguro-safe-driver-prediction/discussion/44629)"
      ]
    },
    {
      "metadata": {
        "id": "zfacosC370yv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "[Caderno do Jeremy para aula 10](https://github.com/fastai/fastai/blob/master/courses/ml1/lesson5-nlp.ipynb) - Lesson5NLP"
      ]
    },
    {
      "metadata": {
        "id": "KuD9G82cqpTE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "[Caderno não-oficial para as aulas do Jeremy](https://forums.fast.ai/t/unofficial-lecture-10-notes/8538)"
      ]
    },
    {
      "metadata": {
        "id": "4auJkjGD3uw2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Vídeo sobre NLP e com o foco na aula 10 de fato começa a partir de 01:02:05 (Simple Linear Model)"
      ]
    },
    {
      "metadata": {
        "id": "_KnOqsDCfRXZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# http://pytorch.org/\n",
        "from os.path import exists\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n",
        "accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n",
        "\n",
        "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.1-{platform}-linux_x86_64.whl torchvision\n",
        "import torch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "o5SGDETjixgI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install fastai==0.7"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EvSbR9URveDr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the \"../input/\" directory.\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
        "\n",
        "import os\n",
        "#print(os.listdir(\"../input\"))\n",
        "#Editar entrada de dados\n",
        "# Any results you write to the current directory are saved as output.\n",
        "\n",
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "#%matplotlib inline\n",
        " \n",
        "from fastai.nlp import *\n",
        "from sklearn.linear_model import LogisticRegression"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-WAYsSmhM6YP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Modelos lineares estão muito perto do estado da arte com o NLP - Jeremy "
      ]
    },
    {
      "metadata": {
        "id": "0pEpDmDA8DYg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Dataset do IMDB será utilizado para Sentiment Classification - Classificação do Sentimento dos Reviews dos filmes. Cerca de 50.000 reviews que contem classificações positivas e negativas. O dataset está dividido em treinamento e teste, com 25.000  reviews classificados para treinamento.  "
      ]
    },
    {
      "metadata": {
        "id": "s46-uZgb9iXv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Rodar os comandos no terminal para acesso ao Dataset: \n",
        "\n",
        "wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "\n",
        "gunzip aclImdb_v1.tar.gz\n",
        "\n",
        "tar -xvf aclImdb_v1.tar\n"
      ]
    },
    {
      "metadata": {
        "id": "Dymk45xPNUOw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!gunzip aclImdb_v1.tar.gz\n",
        "!tar -xvf aclImdb_v1.tar\n",
        "#dataset = wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TyE69gDs5FAq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "PATH='aclImdb/'\n",
        "names = ['neg','pos']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8kG8gPxzTrkg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "No vídeo ele da alguns passos para checar o dataset, mostrando os positivos e negativos, além de um \"ls\" no diretório para mostrar todos os arquivos disponíveis"
      ]
    },
    {
      "metadata": {
        "id": "RvdKNrgy5KDI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "trn,trn_y = texts_labels_from_folders(f'{PATH}train',names)\n",
        "val,val_y = texts_labels_from_folders(f'{PATH}test',names)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3bAkqV4aVCNc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "texts_labels_from_folders??\n",
        "#ele nao gosta de entrar nesse assunto, mas é interessante saber o que acontece na função que foi chamada acima. A funcao vai em cada diretorio e vai em cada arquivo no \n",
        "#diretorio e coloca no array de textos\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "R5p9edayUTti",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "No diretório positivo como exemplo, ele mostra um exemplo do arquivo de texto após a divisão em train/test"
      ]
    },
    {
      "metadata": {
        "id": "ADGGqk1_UXvd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "048bfbcf-9413-4897-97d4-cb40c1c395b4"
      },
      "cell_type": "code",
      "source": [
        "#ATIVIDADE: mostrar o primeiro item do train set, dentro da pasta\n"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"The first look on the cover of this picture, it looks like a good rock n roll movie. But don't let the cover fool you, or the fact that Alice Cooper and Blondie is in it. The storyline is just horrible, and so is the acting. Plain and simple: BAD<br /><br />It's not a movie about a roadie, its just a thin love story, so awful that you see right through it. The only good thing about this movie, is the soundtrack.Some good songs, and that is why I give 2 out of 10. If it wasn't for the music, it would of been 0 out of 10. Meat Loaf is a horrible actor(at least he was in 1980), and the girl who plays the groupie isn't even good looking! This movie was a huge disappointment for me, because it makes a lot of good promises.\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "metadata": {
        "id": "F_nRBuiUUrSl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Após feito acima, ele mostra que na lista de train_y[0] não contem arquivo, pois ainda não foi classificado. "
      ]
    },
    {
      "metadata": {
        "id": "Xh9EuPdbUzvN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "940917d5-9b8b-4c78-e789-036e713ae886"
      },
      "cell_type": "code",
      "source": [
        "trn_y[0]"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "metadata": {
        "id": "PmSH42O7WzLO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Jeremy: \"CountVectorizer converts a collection of text documents to a matrix of token counts (part of sklearn.feature_extraction.text).\""
      ]
    },
    {
      "metadata": {
        "id": "ff00QY8i5MxJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "veczr = CountVectorizer(tokenizer=tokenize)\n",
        "#sklearn - metodo para criar um vetor de palavras"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UBSe1zgjVgdn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Jeremy: \n",
        "\"fit_transform(trn) finds the vocabulary in the training set. It also transforms the training set into a term-document matrix. Since we have to apply the same transformation to your validation set, the second line uses just the method transform(val). trn_term_doc and val_term_doc are sparse matrices. trn_term_doc[i] represents training document i and it contains a count of words for each document for each word in the vocabulary.\"\n",
        "\n",
        "Vídeo: 01:06:50"
      ]
    },
    {
      "metadata": {
        "id": "055Vo5dBXMuN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Na explicação acima também é interessante salientar a etapa da Tokenização, que o Jeremy também explica de forma sensacional. Esta etapa é importante no pré-processamento de NLP. "
      ]
    },
    {
      "metadata": {
        "id": "BU7S5hgX5ZbA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "trn_term_doc = veczr.fit_transform(trn)\n",
        "val_term_doc = veczr.transform(val)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wi1n6VLwaINX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x=trn_term_doc\n",
        "y=trn_y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4spxpuG3Yk50",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Vídeo 01:12:50"
      ]
    },
    {
      "metadata": {
        "id": "au0vDpxhXzn7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "outputId": "0f5e6967-362f-4c39-d677-4b91d678a7fc"
      },
      "cell_type": "code",
      "source": [
        "trn_term_doc\n",
        "#number of unique words - 25.000 \n",
        "#"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-62-7147d2599cdc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrn_term_doc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#number of unique words - 25.000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/scipy/sparse/base.py\u001b[0m in \u001b[0;36m__len__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    244\u001b[0m     \u001b[0;31m# non-zeros is more important.  For now, raise an exception!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 246\u001b[0;31m         raise TypeError(\"sparse matrix length is ambiguous; use getnnz()\"\n\u001b[0m\u001b[1;32m    247\u001b[0m                         \" or shape[0]\")\n\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: sparse matrix length is ambiguous; use getnnz() or shape[0]"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "JjsmmWk0X2fC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 884
        },
        "outputId": "10ed4119-25a2-4400-8bc2-acec1aa30b4b"
      },
      "cell_type": "code",
      "source": [
        "print(trn_term_doc[0])\n",
        "#93 daquelas palavras estao sendo usadas no primeiro documento. "
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  (0, 52090)\t1\n",
            "  (0, 39532)\t1\n",
            "  (0, 40479)\t1\n",
            "  (0, 6430)\t1\n",
            "  (0, 41848)\t1\n",
            "  (0, 18637)\t1\n",
            "  (0, 32000)\t1\n",
            "  (0, 2)\t1\n",
            "  (0, 39416)\t1\n",
            "  (0, 22718)\t1\n",
            "  (0, 34676)\t1\n",
            "  (0, 28865)\t1\n",
            "  (0, 50418)\t1\n",
            "  (0, 73048)\t1\n",
            "  (0, 27577)\t1\n",
            "  (0, 10)\t1\n",
            "  (0, 411)\t1\n",
            "  (0, 72337)\t2\n",
            "  (0, 30228)\t1\n",
            "  (0, 38172)\t1\n",
            "  (0, 4721)\t1\n",
            "  (0, 9)\t1\n",
            "  (0, 1607)\t1\n",
            "  (0, 39168)\t1\n",
            "  (0, 41905)\t1\n",
            "  :\t:\n",
            "  (0, 25406)\t1\n",
            "  (0, 38507)\t1\n",
            "  (0, 65309)\t3\n",
            "  (0, 8)\t4\n",
            "  (0, 19558)\t1\n",
            "  (0, 9854)\t1\n",
            "  (0, 15)\t8\n",
            "  (0, 44202)\t4\n",
            "  (0, 56373)\t1\n",
            "  (0, 44788)\t1\n",
            "  (0, 56253)\t1\n",
            "  (0, 28081)\t5\n",
            "  (0, 1050)\t7\n",
            "  (0, 38797)\t1\n",
            "  (0, 39421)\t1\n",
            "  (0, 34716)\t7\n",
            "  (0, 13)\t10\n",
            "  (0, 49944)\t1\n",
            "  (0, 66684)\t3\n",
            "  (0, 46749)\t5\n",
            "  (0, 15180)\t2\n",
            "  (0, 46986)\t1\n",
            "  (0, 39408)\t1\n",
            "  (0, 24758)\t1\n",
            "  (0, 66458)\t11\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "rsOYsmoVWY3d",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Com isso criamos uma matriz de vetores conhecida como Bag Of Words - BOW. Explicação do Jeremy é sensacional!"
      ]
    },
    {
      "metadata": {
        "id": "ONohtfqyYyXW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Vídeo 01:14:25"
      ]
    },
    {
      "metadata": {
        "id": "StsfW7emYV-z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "vocab = veczr.get_feature_names(); vocab[5000:5005] \n",
        "#exemplo de alguns dos elementos "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3rp8zJUWYbSV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "w0 = set([o.lower() for o in trn[0].split(' ')]); w0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jwXAoCssY6t0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Vídeo 01:14:48 \n",
        "\n",
        "Jeremy explica que não utilizou a função de tokenização do fastai ou do sklearn, mas mostra como ele criou esse tipo de função usando list comprehension.\n",
        "No final, ele tem 91 tokens, diferente do 93 citados acima. E isso é pelo fato dele NÃO ter utilizado o tokenizer. "
      ]
    },
    {
      "metadata": {
        "id": "MUIb18fJZdkO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "len(w0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "65PP6-nQZf9X",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "veczr.vocabulary_['absurd']\n",
        "#aqui ele checa o ID de uma palavra em particular, que deve retornar 1297"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Hep1iMQMZ0ao",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "trn_term_doc[0,1297]\n",
        "#resultado deve ser quantas vezes essa palavra apareceu naquele documento - 2!"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZikfchW_Z--6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "trn_term_doc[0,5000]\n",
        "#0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4LsMPv0T5bQ0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BlZ_50QuXgl3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Naive Bayes"
      ]
    },
    {
      "metadata": {
        "id": "ZAk6txKBfifG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Jeremy: \"Bag of Word approach\"\n",
        "\n",
        "We define the log-count ratio $r$ for each word $f$:\n",
        "\n",
        "$r = \\log \\frac{\\text{ratio of feature $f$ in positive documents}}{\\text{ratio of feature $f$ in negative documents}}$\n",
        "\n",
        "where ratio of feature $f$ in positive documents is the number of times a positive document has a feature divided by the number of positive documents"
      ]
    },
    {
      "metadata": {
        "id": "sqafQl2TNHZ4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def pr(y_i):\n",
        "    p = x[y==y_i].sum(0)\n",
        "    return (p+1) / ((y==y_i).sum()+1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bDv1YZcUf2EG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Vídeo: 01:17:28\n",
        "\n",
        "Jeremy explica muito bem no Excel primeiramente, depois faz o modelo no caderno. \n",
        "\n",
        "Modelo probabilistico. Machine Learning, a column is a feature. "
      ]
    },
    {
      "metadata": {
        "id": "KoEeM289kmdD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Pode ser feito conforme caderno (que utiliza a função acima):"
      ]
    },
    {
      "metadata": {
        "id": "sZc7rvXefgxv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x=trn_term_doc\n",
        "y=trn_y\n",
        "\n",
        "r = np.log(pr(1)/pr(0))\n",
        "b = np.log((y==1).mean() / (y==0).mean())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0uZn1BgFkJTU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Ou, conforme vídeo (01:26:50):"
      ]
    },
    {
      "metadata": {
        "id": "WfIqR7kmkMF7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x=trn_term_doc\n",
        "y=trn_y\n",
        "\n",
        "p = x[y==1].sum(0)+1\n",
        "q = x[y==0].sum(0)+1\n",
        "\n",
        "r = np.log(p/p.sum())/(q/q.sum())\n",
        "b = np.log(len(p)/len(q))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Vl1MaNp3g1ZJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Fórmula para o Naive Bayes"
      ]
    },
    {
      "metadata": {
        "id": "YOkXrnowg5jy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "pre_preds = val_term_doc @ r.T + b\n",
        "preds = pre_preds.T>0\n",
        "(preds==val_y).mean()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VRRf_5tQlSsh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Vídeo 01:29:00"
      ]
    },
    {
      "metadata": {
        "id": "cRELCVSvhxHj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "E para o Naive Bayes de forma binarizada... "
      ]
    },
    {
      "metadata": {
        "id": "XCSs0p6Bh0oK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x=trn_term_doc.sign()\n",
        "r = np.log(pr(1)/pr(0))\n",
        "\n",
        "pre_preds = val_term_doc.sign() @ r.T + b\n",
        "preds = pre_preds.T>0\n",
        "(preds==val_y).mean()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1HlS_OhLlkTX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Logistic Regression\n"
      ]
    },
    {
      "metadata": {
        "id": "LKtL_HbtlpJ6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Agora vamos passar para um modelo um pouco melhor de avaliação - Vídeo 01:30:05"
      ]
    },
    {
      "metadata": {
        "id": "DXEw6DXDl9Sm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "m = LogisticRegression(C=1e8, dual=True)\n",
        "m.fit(x, y)\n",
        "preds = m.predict(val_term_doc)\n",
        "(preds==val_y).mean()\n",
        "\n",
        "#a partir do calculo com regressão logistica - teórico - o resultado final é melhor do que o método anterior. "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "n2x_IGkOl1Ik",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "E uma versão binarizada da Regressão Logística (Vídeo 01:32:20)"
      ]
    },
    {
      "metadata": {
        "id": "DrGEMrN9nEWJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "m = LogisticRegression(C=1e8, dual=True)\n",
        "m.fit(trn_term_doc.sign(), y)\n",
        "preds = m.predict(val_term_doc.sign())\n",
        "(preds==val_y).mean()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GjAS6s-9nHla",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "E uma versão regularizada..."
      ]
    },
    {
      "metadata": {
        "id": "vLTMaYc-nNly",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "m = LogisticRegression(C=0.1, dual=True)\n",
        "m.fit(x, y)\n",
        "preds = m.predict(val_term_doc)\n",
        "(preds==val_y).mean()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6fKjr5MlnO9H",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "E uma versão binarizada da versão regularizada..."
      ]
    },
    {
      "metadata": {
        "id": "0ApF_I-FnWTv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "m = LogisticRegression(C=0.1, dual=True)\n",
        "m.fit(trn_term_doc.sign(), y)\n",
        "preds = m.predict(val_term_doc.sign())\n",
        "(preds==val_y).mean()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "W12giSdfnf2A",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Explicação geral: Vídeo 01:33:45"
      ]
    },
    {
      "metadata": {
        "id": "WFPR5gxYn_Hi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Trigram com Naive Bayes\n"
      ]
    },
    {
      "metadata": {
        "id": "ZJfZLCcooDGS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Vídeo: 01:35:40\n"
      ]
    },
    {
      "metadata": {
        "id": "axfjpyMaoHc4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Jeremy:\n",
        "\n",
        "Our next model is a version of logistic regression with Naive Bayes features described here. For every document we compute binarized features as described above, but this time we use bigrams and trigrams too. Each feature is a log-count ratio. A logistic regression model is then trained to predict sentiment."
      ]
    },
    {
      "metadata": {
        "id": "Pk2I65hfoRbo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "O Sklearn por padrão começa o modelo de vectorização com UniGrams. Podemos alterar a configuração para ele entender BiGrams e TriGrams. (ngram_rang(1,3))\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "UlA0kxiFoGky",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "veczr =  CountVectorizer(ngram_range=(1,3), tokenizer=tokenize, max_features=800000)\n",
        "trn_term_doc = veczr.fit_transform(trn)\n",
        "val_term_doc = veczr.transform(val)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6E561SZ9or7G",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "trn_term_doc.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OxF6zDRBovuX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "vocab = veczr.get_feature_names()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "U2M-v3goowm0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "vocab[200000:200005]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GOjlA3cWoyrS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Apresenta melhores resultados, como o exemplo mostrado no vídeo \"by vast\" e \"by vera miles\""
      ]
    },
    {
      "metadata": {
        "id": "LkQtkO1foqo5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Conforme vídeo:"
      ]
    },
    {
      "metadata": {
        "id": "rGU7qZpvo7_z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "y=trn_y\n",
        "x=trn_term_doc.sign()\n",
        "val_x = val_term_doc.sign()\n",
        "\n",
        "p = x[y==1].sum(0)+1\n",
        "q = x[y==0].sum(0)+1\n",
        "\n",
        "r = np.log(p/p.sum())/(q/q.sum())\n",
        "b = np.log(len(p)/len(q))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-sHsfUAKpSWk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Ou conforme caderno:"
      ]
    },
    {
      "metadata": {
        "id": "_RwPHRbMpUl6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "y=trn_y\n",
        "x=trn_term_doc.sign()\n",
        "val_x = val_term_doc.sign()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hFDPVJGUpWy5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "r = np.log(pr(1) / pr(0))\n",
        "b = np.log((y==1).mean() / (y==0).mean())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zLK1J8FVpkIE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "E aqui fazemos o ajuste para a Regressão Logística com os TriGrams "
      ]
    },
    {
      "metadata": {
        "id": "RdIaETYBpom3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "m = LogisticRegression(C=0.1, dual=True)\n",
        "m.fit(x, y);\n",
        "\n",
        "preds = m.predict(val_x)\n",
        "(preds.T==val_y).mean()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4WKKCv92WJK-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Continua no próximo vídeo...\n"
      ]
    }
  ]
}